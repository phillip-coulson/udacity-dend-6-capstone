{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Project Title\n",
    "### Data Engineering Capstone Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Do all imports and installs here\n",
    "import pandas as pd, re\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf\n",
    "from os import walk, path, listdir\n",
    "from os.path import isfile, join\n",
    "from pyspark.sql.functions import date_add, expr, col, create_map, lit\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, LongType, DoubleType\n",
    "from itertools import chain\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import date_format, upper\n",
    "from pyspark.sql.functions import col, unix_timestamp, to_date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Step 1: Scope the Project and Gather Data\n",
    "## Scope\n",
    "In this project, I plan to perform EDA and build a pipeline to process 4 disparate data sources into an OLAP (Online analytical processing) data layer:\n",
    "1. I94 Immigration Data: This data comes from the US National Tourism and Trade Office\n",
    "2. World Temperature Data: This dataset came from Kaggle\n",
    "3. U.S. City Demographic Data: This data comes from OpenSoft\n",
    "4. Airport Code Table: This is a simple table of airport codes and corresponding cities\n",
    "\n",
    "## Describe and Gather Data\n",
    "The I94 immigration data comes from the US National Tourism and Trade Office, but has been provided as part of this project in a series of SAS files (.sas7bdat). I have also been given a SAS report (I94_SAS_Labels_Descriptions.SAS) that provides valid and invalid values for a number of fields in this dataset (I have manually transformed this report into seperate .txt files for readability)\n",
    "\n",
    "The data contains 28 fields, and there is multiple files with the file directory to consider. but I believe the below are likely to be of most relevance:\n",
    "\n",
    "- cicid - Identifier for immigrating person to the US\n",
    "- arrdate - arrival date\n",
    "- i94port - airport that immigrating person arrived into\n",
    "- count - count of people immigrating with?\n",
    "- airline - airline travelled with\n",
    "- fltno - flight number\n",
    "- i94cit - Country of immigrating person\n",
    "- depdate - Departure date\n",
    "- i94visa - length of visa\n",
    "- occup - occupation\n",
    "- gender - gender\n",
    "- visatype - type of visa\n",
    "- i94addr - US address of immigrating person\n",
    "\n",
    "The historical World Temperature Data comes from Kaggle, but has been provided as part of this project in csv format. The table only contains 7 fields, but I think the below will be most relevant:\n",
    "\n",
    "- AverageTemperature = Average temperature\n",
    "- City = City name\n",
    "- Country = Country name (non US countries are probably not relevant)\n",
    "- Date = Date of temperature\n",
    "\n",
    "The U.S. City Demographic comes from OpenSoft, but has been provided as part of this project in csv format. The table only contains 12 fields, but I think the below will be most relevant: \n",
    "- City - City name                 \n",
    "- State - State name             \n",
    "- Median Age - Median age \n",
    "- Male Population - Male Population size       \n",
    "- Female Population - Female Population size\n",
    "- Foreign-born - Number of foreign born people\n",
    "\n",
    "The Airport Code Table comes from DataHub, but has been provided as part of this project in csv format. The table contains 12 fields, but I think the below will be most relevant:\n",
    "- name - Airport name\n",
    "- type - type of airport\n",
    "- elevation_ft - elevant of airport (in ft)\n",
    "- iso_country - country of airport\n",
    "- iso_region - region of airport\n",
    "- municipality - district of airport\n",
    "\n",
    "There is **no common join key throughout the different data sources**. However, they all relate to locations (location of immigration, location of temperature, location of city, location of airport) therefore I'm hopeful I can find a way to join these data sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "We'll start by checking we can read in all datasets and doing some basic checks to validate the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Exploratory Data Anlaysis (EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Airport Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Read in the airport data form csv into Pandas for exploration\n",
    "fname = './airport-codes_csv.csv'\n",
    "df_airports = pd.read_csv(fname, sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55075"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Start with a simple row count\n",
    "df_airports.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ident</th>\n",
       "      <th>type</th>\n",
       "      <th>name</th>\n",
       "      <th>elevation_ft</th>\n",
       "      <th>continent</th>\n",
       "      <th>iso_country</th>\n",
       "      <th>iso_region</th>\n",
       "      <th>municipality</th>\n",
       "      <th>gps_code</th>\n",
       "      <th>iata_code</th>\n",
       "      <th>local_code</th>\n",
       "      <th>coordinates</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00A</td>\n",
       "      <td>heliport</td>\n",
       "      <td>Total Rf Heliport</td>\n",
       "      <td>11.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-PA</td>\n",
       "      <td>Bensalem</td>\n",
       "      <td>00A</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00A</td>\n",
       "      <td>-74.93360137939453, 40.07080078125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00AA</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Aero B Ranch Airport</td>\n",
       "      <td>3435.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-KS</td>\n",
       "      <td>Leoti</td>\n",
       "      <td>00AA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00AA</td>\n",
       "      <td>-101.473911, 38.704022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00AK</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Lowell Field</td>\n",
       "      <td>450.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-AK</td>\n",
       "      <td>Anchor Point</td>\n",
       "      <td>00AK</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00AK</td>\n",
       "      <td>-151.695999146, 59.94919968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00AL</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Epps Airpark</td>\n",
       "      <td>820.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-AL</td>\n",
       "      <td>Harvest</td>\n",
       "      <td>00AL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00AL</td>\n",
       "      <td>-86.77030181884766, 34.86479949951172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00AR</td>\n",
       "      <td>closed</td>\n",
       "      <td>Newport Hospital &amp; Clinic Heliport</td>\n",
       "      <td>237.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-AR</td>\n",
       "      <td>Newport</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-91.254898, 35.6087</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ident           type                                name  elevation_ft  \\\n",
       "0   00A       heliport                   Total Rf Heliport          11.0   \n",
       "1  00AA  small_airport                Aero B Ranch Airport        3435.0   \n",
       "2  00AK  small_airport                        Lowell Field         450.0   \n",
       "3  00AL  small_airport                        Epps Airpark         820.0   \n",
       "4  00AR         closed  Newport Hospital & Clinic Heliport         237.0   \n",
       "\n",
       "  continent iso_country iso_region  municipality gps_code iata_code  \\\n",
       "0       NaN          US      US-PA      Bensalem      00A       NaN   \n",
       "1       NaN          US      US-KS         Leoti     00AA       NaN   \n",
       "2       NaN          US      US-AK  Anchor Point     00AK       NaN   \n",
       "3       NaN          US      US-AL       Harvest     00AL       NaN   \n",
       "4       NaN          US      US-AR       Newport      NaN       NaN   \n",
       "\n",
       "  local_code                            coordinates  \n",
       "0        00A     -74.93360137939453, 40.07080078125  \n",
       "1       00AA                 -101.473911, 38.704022  \n",
       "2       00AK            -151.695999146, 59.94919968  \n",
       "3       00AL  -86.77030181884766, 34.86479949951172  \n",
       "4        NaN                    -91.254898, 35.6087  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data has been read in without any errors, display first few rows\n",
    "df_airports.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ident           55075\n",
       "type            55075\n",
       "name            55075\n",
       "elevation_ft    48069\n",
       "continent       27356\n",
       "iso_country     54828\n",
       "iso_region      55075\n",
       "municipality    49399\n",
       "gps_code        41030\n",
       "iata_code        9189\n",
       "local_code      28686\n",
       "coordinates     55075\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Continent and iata_code appear to have lots of NaN values. A summary of the tabel will show how well populated each of the fields are\n",
    "df_airports.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# continent, iata_code and local_code look to be poorly populated, so may not be fit for use (but are out of my scope). Check for duplication (assuming each airport has a unqiue 'ident' key)\n",
    "len(df_airports[['ident']])-len(df_airports[['ident']].drop_duplicates())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ident            object\n",
       "type             object\n",
       "name             object\n",
       "elevation_ft    float64\n",
       "continent        object\n",
       "iso_country      object\n",
       "iso_region       object\n",
       "municipality     object\n",
       "gps_code         object\n",
       "iata_code        object\n",
       "local_code       object\n",
       "coordinates      object\n",
       "dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ident look to be the primary key. Check datatypes have been assigned correctly\n",
    "df_airports.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Datatypes have not been assigned correctly and most numeric fields are showing as object/string. I will need to consider this as part of the data cleansing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Findings from airport data:\n",
    "- ident is the primary key\n",
    "- continent, iata_code and local_code but are not included in my scope\n",
    "- Pandas cannot infer the datatypes correctly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Cities Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Read in the cities data from csv into Pandas for exploration\n",
    "fname = './us-cities-demographics.csv'\n",
    "df_cities = pd.read_csv(fname, sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2891"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Start with a simple row count\n",
    "df_cities.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>City</th>\n",
       "      <th>State</th>\n",
       "      <th>Median Age</th>\n",
       "      <th>Male Population</th>\n",
       "      <th>Female Population</th>\n",
       "      <th>Total Population</th>\n",
       "      <th>Number of Veterans</th>\n",
       "      <th>Foreign-born</th>\n",
       "      <th>Average Household Size</th>\n",
       "      <th>State Code</th>\n",
       "      <th>Race</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Silver Spring</td>\n",
       "      <td>Maryland</td>\n",
       "      <td>33.8</td>\n",
       "      <td>40601.0</td>\n",
       "      <td>41862.0</td>\n",
       "      <td>82463</td>\n",
       "      <td>1562.0</td>\n",
       "      <td>30908.0</td>\n",
       "      <td>2.60</td>\n",
       "      <td>MD</td>\n",
       "      <td>Hispanic or Latino</td>\n",
       "      <td>25924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Quincy</td>\n",
       "      <td>Massachusetts</td>\n",
       "      <td>41.0</td>\n",
       "      <td>44129.0</td>\n",
       "      <td>49500.0</td>\n",
       "      <td>93629</td>\n",
       "      <td>4147.0</td>\n",
       "      <td>32935.0</td>\n",
       "      <td>2.39</td>\n",
       "      <td>MA</td>\n",
       "      <td>White</td>\n",
       "      <td>58723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hoover</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>38.5</td>\n",
       "      <td>38040.0</td>\n",
       "      <td>46799.0</td>\n",
       "      <td>84839</td>\n",
       "      <td>4819.0</td>\n",
       "      <td>8229.0</td>\n",
       "      <td>2.58</td>\n",
       "      <td>AL</td>\n",
       "      <td>Asian</td>\n",
       "      <td>4759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Rancho Cucamonga</td>\n",
       "      <td>California</td>\n",
       "      <td>34.5</td>\n",
       "      <td>88127.0</td>\n",
       "      <td>87105.0</td>\n",
       "      <td>175232</td>\n",
       "      <td>5821.0</td>\n",
       "      <td>33878.0</td>\n",
       "      <td>3.18</td>\n",
       "      <td>CA</td>\n",
       "      <td>Black or African-American</td>\n",
       "      <td>24437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Newark</td>\n",
       "      <td>New Jersey</td>\n",
       "      <td>34.6</td>\n",
       "      <td>138040.0</td>\n",
       "      <td>143873.0</td>\n",
       "      <td>281913</td>\n",
       "      <td>5829.0</td>\n",
       "      <td>86253.0</td>\n",
       "      <td>2.73</td>\n",
       "      <td>NJ</td>\n",
       "      <td>White</td>\n",
       "      <td>76402</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               City          State  Median Age  Male Population  \\\n",
       "0     Silver Spring       Maryland        33.8          40601.0   \n",
       "1            Quincy  Massachusetts        41.0          44129.0   \n",
       "2            Hoover        Alabama        38.5          38040.0   \n",
       "3  Rancho Cucamonga     California        34.5          88127.0   \n",
       "4            Newark     New Jersey        34.6         138040.0   \n",
       "\n",
       "   Female Population  Total Population  Number of Veterans  Foreign-born  \\\n",
       "0            41862.0             82463              1562.0       30908.0   \n",
       "1            49500.0             93629              4147.0       32935.0   \n",
       "2            46799.0             84839              4819.0        8229.0   \n",
       "3            87105.0            175232              5821.0       33878.0   \n",
       "4           143873.0            281913              5829.0       86253.0   \n",
       "\n",
       "   Average Household Size State Code                       Race  Count  \n",
       "0                    2.60         MD         Hispanic or Latino  25924  \n",
       "1                    2.39         MA                      White  58723  \n",
       "2                    2.58         AL                      Asian   4759  \n",
       "3                    3.18         CA  Black or African-American  24437  \n",
       "4                    2.73         NJ                      White  76402  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data has been read in without any errors, display first few rows\n",
    "df_cities.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "City                      2891\n",
       "State                     2891\n",
       "Median Age                2891\n",
       "Male Population           2888\n",
       "Female Population         2888\n",
       "Total Population          2891\n",
       "Number of Veterans        2878\n",
       "Foreign-born              2878\n",
       "Average Household Size    2875\n",
       "State Code                2891\n",
       "Race                      2891\n",
       "Count                     2891\n",
       "dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data look to be of reasonable quality. Summary check will confirm this\n",
    "df_cities.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2295"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for duplication (assuming a city name could be used in more than 1 state?)\n",
    "len(df_cities[['City', 'State']])-len(df_cities[['City', 'State']].drop_duplicates())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "City                       object\n",
       "State                      object\n",
       "Median Age                float64\n",
       "Male Population           float64\n",
       "Female Population         float64\n",
       "Total Population            int64\n",
       "Number of Veterans        float64\n",
       "Foreign-born              float64\n",
       "Average Household Size    float64\n",
       "State Code                 object\n",
       "Race                       object\n",
       "Count                       int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# There definitely looks to be a lot of duplication in this data. So will need to dedup as part of the data cleansing\n",
    "# City and State should be the primary key. Check datatypes have been assigned correctly\n",
    "df_cities.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Findings from cities data:\n",
    "- All fields are well populated\n",
    "- Dataset contains a large number of duplicates\n",
    "- Pandas can infer the schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Global Land Temperature (by City) Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Read in the temperature data from csv into Pandas for exploration\n",
    "fname = '../../data2/GlobalLandTemperaturesByCity.csv'\n",
    "df_temperature = pd.read_csv(fname, sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8599212"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Start with a simple row count\n",
    "df_temperature.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dt</th>\n",
       "      <th>AverageTemperature</th>\n",
       "      <th>AverageTemperatureUncertainty</th>\n",
       "      <th>City</th>\n",
       "      <th>Country</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1743-11-01</td>\n",
       "      <td>6.068</td>\n",
       "      <td>1.737</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1743-12-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1744-01-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1744-02-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1744-03-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           dt  AverageTemperature  AverageTemperatureUncertainty   City  \\\n",
       "0  1743-11-01               6.068                          1.737  Århus   \n",
       "1  1743-12-01                 NaN                            NaN  Århus   \n",
       "2  1744-01-01                 NaN                            NaN  Århus   \n",
       "3  1744-02-01                 NaN                            NaN  Århus   \n",
       "4  1744-03-01                 NaN                            NaN  Århus   \n",
       "\n",
       "   Country Latitude Longitude  \n",
       "0  Denmark   57.05N    10.33E  \n",
       "1  Denmark   57.05N    10.33E  \n",
       "2  Denmark   57.05N    10.33E  \n",
       "3  Denmark   57.05N    10.33E  \n",
       "4  Denmark   57.05N    10.33E  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data has been read in without any errors, display first few entries\n",
    "df_temperature.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dt                               8599212\n",
       "AverageTemperature               8235082\n",
       "AverageTemperatureUncertainty    8235082\n",
       "City                             8599212\n",
       "Country                          8599212\n",
       "Latitude                         8599212\n",
       "Longitude                        8599212\n",
       "dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Temperature fields (AverageTemperature and Average TemperatureUncertainty) appear to have lots of NaN values. These will likely need filtering out as part of data cleansing. A summary of the tabel will show how well populated each of the fields are\n",
    "df_temperature.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique cities with NaN is:3323\n",
      "Number of unique dates with NaN is:1772\n",
      "Number of unique Countries with NaN is:157\n"
     ]
    }
   ],
   "source": [
    "# The above confirms that the temperature fields have 4% null values (not drastic, but the data is irrelevant for me without this). Quick check if this relates to a particular city/date/country that I don't need\n",
    "notnull_temperature_df = df_temperature[df_temperature['AverageTemperature'].isnull()]\n",
    "\n",
    "unique_city_num = notnull_temperature_df['City'].unique().shape[0]\n",
    "unique_date_num = notnull_temperature_df['dt'].unique().shape[0]\n",
    "unique_country_num = notnull_temperature_df['Country'].unique().shape[0]\n",
    "\n",
    "print('Number of unique cities with NaN is:' + str(unique_city_num))\n",
    "print('Number of unique dates with NaN is:' + str(unique_date_num))\n",
    "print('Number of unique Countries with NaN is:' + str(unique_country_num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Doesn't look to be a specific issue and more of a general DQ issue that I should filter out in data cleansing. The remaining fields look to be well populated, but I'll an additional check to confirm that there aren't duplicates to consider\n",
    "len(df_temperature[['dt','City','Country','Latitude','Longitude']])-len(df_temperature[['dt','City','Country','Latitude','Longitude']].drop_duplicates())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AverageTemperature</th>\n",
       "      <th>AverageTemperatureUncertainty</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>8.235082e+06</td>\n",
       "      <td>8.235082e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.672743e+01</td>\n",
       "      <td>1.028575e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.035344e+01</td>\n",
       "      <td>1.129733e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-4.270400e+01</td>\n",
       "      <td>3.400000e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.029900e+01</td>\n",
       "      <td>3.370000e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.883100e+01</td>\n",
       "      <td>5.910000e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2.521000e+01</td>\n",
       "      <td>1.349000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>3.965100e+01</td>\n",
       "      <td>1.539600e+01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       AverageTemperature  AverageTemperatureUncertainty\n",
       "count        8.235082e+06                   8.235082e+06\n",
       "mean         1.672743e+01                   1.028575e+00\n",
       "std          1.035344e+01                   1.129733e+00\n",
       "min         -4.270400e+01                   3.400000e-02\n",
       "25%          1.029900e+01                   3.370000e-01\n",
       "50%          1.883100e+01                   5.910000e-01\n",
       "75%          2.521000e+01                   1.349000e+00\n",
       "max          3.965100e+01                   1.539600e+01"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# No duplication. A final describe may provide some final insight\n",
    "df_temperature.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dt                               687289\n",
       "AverageTemperature               661524\n",
       "AverageTemperatureUncertainty    661524\n",
       "City                             687289\n",
       "Country                          687289\n",
       "Latitude                         687289\n",
       "Longitude                        687289\n",
       "dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We're only actually interested in temperatures for the US, find corresponding value\n",
    "df_temperature['Country'].unique()\n",
    "# Country == 'United States'\n",
    "df_US = df_temperature[df_temperature['Country']=='United States'].count()\n",
    "df_US"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Global Temperature data observations:\n",
    "- some minor data quality issues with the temperature fields that should probably be filtered out. \n",
    "- The other fields seem fine and there is no identified duplication. \n",
    "- Once filtered down for united states, the dataset isn't too large. But may benefit from parallel processing (e.g. Spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### I94 Immegration Data\n",
    "It has already been suggested that this data is quite large, and so it will be best to use Spark to perform any EDA for this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Create Spark session with SAS7BDAT jar\n",
    "spark = SparkSession\\\n",
    ".builder \\\n",
    ".config(\"spark.jars.packages\",\"saurfang:spark-sas7bdat:2.0.0-s_2.11\") \\\n",
    ".enableHiveSupport().getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i94_apr16_sub.sas7bdat', 'i94_sep16_sub.sas7bdat', 'i94_nov16_sub.sas7bdat', 'i94_mar16_sub.sas7bdat', 'i94_jun16_sub.sas7bdat', 'i94_aug16_sub.sas7bdat', 'i94_may16_sub.sas7bdat', 'i94_jan16_sub.sas7bdat', 'i94_oct16_sub.sas7bdat', 'i94_jul16_sub.sas7bdat', 'i94_feb16_sub.sas7bdat', 'i94_dec16_sub.sas7bdat']\n"
     ]
    }
   ],
   "source": [
    "# List of files in directory\n",
    "immegration_path = '../../data/18-83510-I94-Data-2016'\n",
    "immegration_files = [f for f in listdir(immegration_path) if isfile(join(immegration_path, f))]\n",
    "print(immegration_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# There are monthly files for the I94 data, we will just examine one at this stage\n",
    "immigration_data = path.join(immegration_path,immegration_files[0])\n",
    "df_immigration = spark.read.format('com.github.saurfang.sas.spark').load(immigration_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3096313"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Start with a simple row count\n",
    "df_immigration.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+--------------+-----+--------+\n",
      "|cicid| i94yr|i94mon|i94cit|i94res|i94port|arrdate|i94mode|i94addr|depdate|i94bir|i94visa|count|dtadfile|visapost|occup|entdepa|entdepd|entdepu|matflag|biryear| dtaddto|gender|insnum|airline|        admnum|fltno|visatype|\n",
      "+-----+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+--------------+-----+--------+\n",
      "|  6.0|2016.0|   4.0| 692.0| 692.0|    XXX|20573.0|   null|   null|   null|  37.0|    2.0|  1.0|    null|    null| null|      T|   null|      U|   null| 1979.0|10282016|  null|  null|   null| 1.897628485E9| null|      B2|\n",
      "|  7.0|2016.0|   4.0| 254.0| 276.0|    ATL|20551.0|    1.0|     AL|   null|  25.0|    3.0|  1.0|20130811|     SEO| null|      G|   null|      Y|   null| 1991.0|     D/S|     M|  null|   null|  3.73679633E9|00296|      F1|\n",
      "| 15.0|2016.0|   4.0| 101.0| 101.0|    WAS|20545.0|    1.0|     MI|20691.0|  55.0|    2.0|  1.0|20160401|    null| null|      T|      O|   null|      M| 1961.0|09302016|     M|  null|     OS|  6.66643185E8|   93|      B2|\n",
      "| 16.0|2016.0|   4.0| 101.0| 101.0|    NYC|20545.0|    1.0|     MA|20567.0|  28.0|    2.0|  1.0|20160401|    null| null|      O|      O|   null|      M| 1988.0|09302016|  null|  null|     AA|9.246846133E10|00199|      B2|\n",
      "| 17.0|2016.0|   4.0| 101.0| 101.0|    NYC|20545.0|    1.0|     MA|20567.0|   4.0|    2.0|  1.0|20160401|    null| null|      O|      O|   null|      M| 2012.0|09302016|  null|  null|     AA|9.246846313E10|00199|      B2|\n",
      "+-----+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+--------------+-----+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Given that there is 12 months of files, we can expect there to be around 36m rows of I94 data. \n",
    "# Data has been read in without any errors, display first few entries\n",
    "df_immigration.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Given the size and nature of this data. I'd expect there to be some null values.\n",
    "# In I94_SAS_Labels_Descriptions.SAS, we can see that there is expected invalid values in the columns i94addr, i94port, i94cit & i94res. These should be filtered out during the data cleansing\n",
    "# Check for duplicates (assume cicid is the primary key)\n",
    "df_immigration.count() - df_immigration.dropDuplicates(['cicid']).count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# There looks to be duplicate values in this data that should be filtered out "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Immigration Data Findings:\n",
    "- There could be around 36m rows of this data to process, definitely warrents using spark for parralel efficiencies\n",
    "- There are multiple files to consider\n",
    "- There are invalid values that need filtering out of the data\n",
    "- The date columns are in SAS data format (number of days since 1st Jan 1960)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Step 2: Clean the Data and Data Tranformation\n",
    "I plan to perform the following data cleaning.\n",
    "\n",
    "For the I94 immigration data:\n",
    "- DROP rows where i94port do not contain a valid value\n",
    "- DROP rows where i94cit or i94 res do not contain valid values\n",
    "- DROP rows where i94addr does not contain valid values\n",
    "- change date columns from SAS dates to readable dates\n",
    "- get dictionary values rather than keys where possible\n",
    "- Derive a CITY and REGION code as foreign keys for temperature, cities and airport data \n",
    "\n",
    "For the temperature data:\n",
    "- DROP rows where AverageTemperature is NaN\n",
    "- DROP rows where country is not United States\n",
    "- Upper case STRING columns that may be required as keys\n",
    "- Cast dates to date format for readability\n",
    "\n",
    "For the cities data:\n",
    "- DROP duplicate rows based on City and State\n",
    "- Assign a valid schema\n",
    "- Upper case STRING columns that may be required as keys\n",
    "\n",
    "For the airport data:\n",
    "- Assign a valid schema\n",
    "- Derive a REGION code as foreign keys for immigrant data \n",
    "\n",
    "Note, dictionaries have been created manually for i94addrl, i94cntyl, i94prtl. These contain the valid values from I94_SAS_Labels_Descriptions.SAS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### i94 Immigration Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_valid_codes(filepath: str, delim:str =\"=\"):\n",
    "    \"\"\"\n",
    "    Reads a csv/txt dictionary file and transforms to dictionary\n",
    "    \n",
    "    args:\n",
    "        filepath: file name & path\n",
    "        delim: delimeter\n",
    "        \n",
    "    returns:\n",
    "        dictionary\n",
    "    \"\"\"\n",
    "    # Read txt file\n",
    "    valid_df = pd.read_csv(filepath, sep='=', header=None)\n",
    "    # convert df to dictionary \n",
    "    valid_dict = dict(zip(valid_df[0], valid_df[1]))\n",
    "    return valid_dict\n",
    "\n",
    "def read_and_clean_i94_data(filepath: str):\n",
    "    \"\"\"\n",
    "    reads a single i94 SAS file and performs data cleansing:\n",
    "        - DROP rows where i94port do not contain a valid value\n",
    "        - DROP rows where i94cit or i94 res do not contain valid values\n",
    "        - DROP rows where i94addr does not contain valid values\n",
    "        - change date columns from SAS dates to readable dates\n",
    "        - get dictionary values rather than keys where possible\n",
    "        - Derive a CITY and REGION code as foreign keys for temperature, cities and airport data\n",
    "    \n",
    "    args:\n",
    "        filepath: file name & path\n",
    "        \n",
    "    returns:\n",
    "        Spark DataFrame\n",
    "    \"\"\"\n",
    "    \n",
    "    # get valid code dictionaries\n",
    "    i94port_dict = get_valid_codes('./i94prtl_valid.txt')\n",
    "    i94cit_i94res_dict = get_valid_codes('./i94cntyl_valid.txt')\n",
    "    i94addr_dict = get_valid_codes('./i94addrl_valid.txt')\n",
    "    \n",
    "    # map i94port values\n",
    "    mapping_expr = create_map([lit(x) for x in chain(*i94port_dict.items())])\n",
    "    \n",
    "    # Read I94 data into Spark\n",
    "    df_immigration = spark.read.format('com.github.saurfang.sas.spark').load(filepath)\n",
    "\n",
    "    # Select required columns and filter invalid codes\n",
    "    df_immigration = (\n",
    "        df_immigration.select(\n",
    "            [\n",
    "                'cicid',\n",
    "                'arrdate',\n",
    "                'i94port',\n",
    "                col('count').alias('cnt'),\n",
    "                'airline', \n",
    "                'fltno',\n",
    "                'i94cit', \n",
    "                'depdate',\n",
    "                'i94visa',\n",
    "                'occup',\n",
    "                'gender',\n",
    "                'visatype',\n",
    "                'i94addr',\n",
    "                \n",
    "            ]\n",
    "        )\n",
    "        # Filter out entries where i94port is invalid\n",
    "        .filter(df_immigration.i94port.isin(list(i94port_dict.keys())))\n",
    "        # Filter out entries where i94cit is invalid\n",
    "        .filter(df_immigration.i94cit.isin(list(i94cit_i94res_dict.keys())))\n",
    "        # Filter out entries where i94res is invalid\n",
    "        .filter(df_immigration.i94res.isin(list(i94cit_i94res_dict.keys())))\n",
    "        # Filter out entries where i94addr is invalid\n",
    "        .filter(df_immigration.i94addr.isin(list(i94addr_dict.keys())))\n",
    "        # Convert SAS dates to meaningful dates\n",
    "        .withColumn('arrdate', expr(\"date_add(to_date('1960-01-01'), arrdate)\"))\n",
    "        .withColumn('depdate', expr(\"date_add(to_date('1960-01-01'), depdate)\"))\n",
    "        # Lookup i94port to get City and Region\n",
    "        .withColumn(\"CityRegion\", mapping_expr.getItem(col(\"i94port\")))\n",
    "    )\n",
    "    \n",
    "    # Create CITY and REGION keys\n",
    "    df_immigration = (\n",
    "        df_immigration\n",
    "        .withColumn(\"City\", expr(\"trim(split(CityRegion, ',')[0])\"))\n",
    "        .withColumn(\"Region\", expr(\"trim(split(CityRegion, ',')[1])\"))\n",
    "        .drop(\"CityRegion\")\n",
    "    )\n",
    "    \n",
    "    return df_immigration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### temperature Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def read_and_clean_temperature_data(filepath: str):\n",
    "    \"\"\"\n",
    "    reads temperature csv file and performs data cleansing:\n",
    "        - DROP rows where AverageTemperature is NaN\n",
    "        - DROP rows where country is not United States\n",
    "        - Upper case STRING columns that may be required as keys\n",
    "        - Cast dates to date format for readability\n",
    "    \n",
    "    args:\n",
    "        filepath: file name & path\n",
    "        \n",
    "    returns:\n",
    "        Spark DataFrame\n",
    "    \"\"\"    \n",
    "    # Read temperature data into Spark\n",
    "    df_temperature = (\n",
    "        spark.read.format(\"csv\") \n",
    "        .option(\"header\",True)\n",
    "        .load(filepath)  \n",
    "    )\n",
    "    \n",
    "    # Filter missing temperatures and United States Cities\n",
    "    df_temperature = df_temperature.filter(\n",
    "        \"AverageTemperature IS NOT NULL AND Country == 'United States'\"\n",
    "    )\n",
    "    \n",
    "    \n",
    "    # Covert dt to readable date and reorder fields\n",
    "    df_temperature = (\n",
    "        df_temperature.select(['Country', 'City', 'dt', 'AverageTemperature']) \\\n",
    "        .withColumn('dt', to_date(unix_timestamp(col('dt'), 'yyyy-MM-dd').cast(\"timestamp\")))\n",
    "        .withColumn('Country', upper(col('Country')))\n",
    "        .withColumn('City', upper(col('City')))\n",
    "    )\n",
    "    \n",
    "    return df_temperature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Cities Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def read_and_clean_cities_data(filepath: str):\n",
    "    \"\"\"\n",
    "    reads cities csv file and performs data cleansing:\n",
    "        -DROP duplicate rows based on City and State\n",
    "        -Assign a valid schema\n",
    "        -Upper case STRING columns that may be required as keys\n",
    "    \n",
    "    args:\n",
    "        filepath: file name & path\n",
    "        \n",
    "    returns:\n",
    "        Spark DataFrame\n",
    "    \"\"\" \n",
    "    \n",
    "    # Define schema\n",
    "    citySchema = StructType(\n",
    "        [\n",
    "            StructField('City', StringType(), True),\n",
    "            StructField('State', StringType(), True),\n",
    "            StructField('MedianAge', DoubleType(), True),\n",
    "            StructField('MalePopulation', LongType(), True),\n",
    "            StructField('FemalePopulation', LongType(), True),\n",
    "            StructField('NumberOfVeterans', LongType(), True),\n",
    "            StructField('Foreign-born', StringType(), True),     \n",
    "            StructField('AverageHouseholdSize', IntegerType(), True),\n",
    "            StructField('Race', StringType(), True),\n",
    "            StructField('Count', StringType(), True)\n",
    "        ]                     \n",
    "    )\n",
    "    \n",
    "    # Read cities data into Spark and assign schema\n",
    "    df_cities = (\n",
    "        spark.read.format(\"csv\") \n",
    "        .option(\"header\",True)\n",
    "        .option(\"delimiter\", \";\")\n",
    "        .schema(citySchema)\n",
    "        .load(filepath)  \n",
    "        .select(\n",
    "            ['City','State','MedianAge','MalePopulation','FemalePopulation','Foreign-born']\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Drop Duplicates and UPPER case keys\n",
    "    df_cities = (\n",
    "        df_cities\n",
    "        .withColumn('City', upper(col('City')))\n",
    "        .withColumn('State', upper(col('State')))\n",
    "        .drop_duplicates(subset=['City', 'State'])\n",
    "    )\n",
    "    \n",
    "    return df_cities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Airport Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def read_and_clean_airport_data(filepath: str):\n",
    "    \"\"\"\n",
    "    reads airport csv file and performs data cleansing:\n",
    "        -Assign a valid schema\n",
    "        -Derive a REGION code as foreign keys for immigrant data\n",
    "    args:\n",
    "        filepath: file name & path\n",
    "        \n",
    "    returns:\n",
    "        Spark DataFrame\n",
    "    \"\"\"     \n",
    "    \n",
    "    # Define schema    \n",
    "    airportSchema = StructType(\n",
    "        [\n",
    "            StructField('ident', StringType(), True),\n",
    "            StructField('type', StringType(), True),\n",
    "            StructField('name', StringType(), True),\n",
    "            StructField('elevation_ft', IntegerType(), True),\n",
    "            StructField('continent', StringType(), True),\n",
    "            StructField('iso_country', StringType(), True),\n",
    "            StructField('iso_region', StringType(), True),     \n",
    "            StructField('municipality', StringType(), True),\n",
    "            StructField('gps_code', StringType(), True),\n",
    "            StructField('iata_code', StringType(), True),\n",
    "            StructField('local_code', StringType(), True),\n",
    "            StructField('coordinates', StringType(), True),\n",
    "        ]                     \n",
    "    )\n",
    "    \n",
    "    # Read cities data into Spark and assign schema\n",
    "    df_airport = (\n",
    "        spark.read.format(\"csv\") \n",
    "        .option(\"header\",True)\n",
    "        .schema(airportSchema)\n",
    "        .load(filepath)  \n",
    "        .select(\n",
    "            ['name','type','elevation_ft','iso_country','iso_region','municipality']\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Derive REGION key\n",
    "    df_airport = (\n",
    "        df_airport\n",
    "        .withColumn(\"Region\", expr(\"trim(split(iso_region, '-')[1])\"))\n",
    "        .drop(\"iso_region\")\n",
    "    )\n",
    "    \n",
    "    return df_airport"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Step 3: Define the Data Model\n",
    "## 3.1 Conceptual Data Model\n",
    "\n",
    "I have decided to create 5 tables based on a STAR schema design.\n",
    "\n",
    "- The FACT table will a record of people immigrating into the United States and high level attributes\n",
    "- There will then be 4 DIMENSION tables that will contain additional attributes about:\n",
    "    - The people immigrating\n",
    "    - The airports they're immigrating to\n",
    "    - The temperature where they're immigrating to\n",
    "    - The cities they're immigrating to\n",
    "    \n",
    "The IMMIGRATION_FACT fact table will contain the events from the I94 immigration data (people immigrating to the United States). The data dictionary and lineage for this table will be will be:\n",
    "\n",
    "| Table            | Variable         | DataType    | Source File             | Source Variable  | Description           |\n",
    "| ---------------- | ---------------- | ----------- | ----------------------- | ---------------- | --------------------- |\n",
    "| IMMIGRATION_FACT | cicid \t\t\t  | STRING      | i94_<MONYY>_sub.sas7bdat| cicid \t\t\t | Citizen ID            |\n",
    "| IMMIGRATION_FACT | arrdate          | STRING      | i94_<MONYY>_sub.sas7bdat| arrdate          | Arrival Date          |\n",
    "| IMMIGRATION_FACT | i94port          | DOUBLE      | i94_<MONYY>_sub.sas7bdat| i94port          | Arrival Airport       |\n",
    "| IMMIGRATION_FACT | count            | LONG        | i94_<MONYY>_sub.sas7bdat| count            | Arrival Count         |\n",
    "| IMMIGRATION_FACT | airline          | LONG        | i94_<MONYY>_sub.sas7bdat| airline          | Airline Name          |\n",
    "| IMMIGRATION_FACT | fltno            | STRING      | i94_<MONYY>_sub.sas7bdat| fltno            | Flight Number         |\n",
    "| IMMIGRATION_FACT | i94cit           | STRING      | i94_<MONYY>_sub.sas7bdat| i94cit           | Immigrating Country   |\n",
    "\n",
    "The IMMIGRANT_DIM dimension table will contain additional immigrant level attibutes. The data dictionary and lineage for this table will be will be:\n",
    "\n",
    "| Table            | Variable         | DataType    | Source File             | Source Variable  | Description           |\n",
    "| ---------------- | ---------------- | ----------- | ----------------------- | ---------------- | --------------------- |\n",
    "| IMMIGRATION_DIM  | cicid \t\t\t  | STRING      | i94_<MONYY>_sub.sas7bdat| cicid \t\t\t | Citizen ID            |\n",
    "| IMMIGRATION_DIM  | arrdate          | STRING      | i94_<MONYY>_sub.sas7bdat| arrdate          | Arrival Date          |\n",
    "| IMMIGRATION_DIM  | i94port          | DOUBLE      | i94_<MONYY>_sub.sas7bdat| i94port          | Arrival Airport       |\n",
    "| IMMIGRATION_DIM  | depdate          | STRING      | i94_<MONYY>_sub.sas7bdat| depdate          | Departure Date        |\n",
    "| IMMIGRATION_DIM  | i94visa          | STRING      | i94_<MONYY>_sub.sas7bdat| i94visa          | Visa Type 1           |\n",
    "| IMMIGRATION_DIM  | occup            | STRING      | i94_<MONYY>_sub.sas7bdat| occup            | Occupation            |\n",
    "| IMMIGRATION_DIM  | gender           | STRING      | i94_<MONYY>_sub.sas7bdat| gender           | Gender                |\n",
    "| IMMIGRATION_DIM  | visatype         | STRING      | i94_<MONYY>_sub.sas7bdat| visatype         | Visa Type 2           |\n",
    "| IMMIGRATION_DIM  | i94addr          | STRING      | i94_<MONYY>_sub.sas7bdat| i94addr          | Immigrating Address   |\n",
    "\n",
    "The CITIES_DIM table will be a dimension table contains cities and city level attributes. The data dictionary and lineage for this table will be will be:\n",
    "\n",
    "| Table            | Variable         | DataType    | Source File               | Source Variable  | Description           |\n",
    "| ---------------- | ---------------- | ----------- | ------------------------- | ---------------- | --------------------- |\n",
    "| CITY_DIM         | City             | STRING      | us-cities-demographics.csv| City             | City                  |\n",
    "| CITY_DIM         | State            | STRING      | us-cities-demographics.csv| State            | State                 |\n",
    "| CITY_DIM         | MedianAge        | DOUBLE      | us-cities-demographics.csv| MedianAge        | Median Age            |\n",
    "| CITY_DIM         | MalePopulation   | LONG        | us-cities-demographics.csv| MalePopulation   | Male Population Count |\n",
    "| CITY_DIM         | FemalePopulation | LONG        | us-cities-demographics.csv| FemalePopulation | FemalePopulation      |\n",
    "| CITY_DIM         | Foreign-born     | STRING      | us-cities-demographics.csv| Foreign-born     | Foreign-born          |\n",
    "\n",
    "\n",
    "The TEMPERATUTE_DIM dimension will be a dimension table containing historic temperatures corresponding to a City. The data dictionary and lineage for this table will be will be:\n",
    "\n",
    "| Table            | Variable           | DataType    | Source File                     | Source Variable    | Description           |\n",
    "| ---------------- | ------------------ | ----------- | ------------------------------- | ------------------ | --------------------- |\n",
    "| TEMPERATURE_DIM  | Country            | STRING      | GlobalLandTemperaturesByCity.csv| Country            | Country               |\n",
    "| TEMPERATURE_DIM  | dt                 | DATE        | GlobalLandTemperaturesByCity.csv| dt                 | date                  |\n",
    "| TEMPERATURE_DIM  | AverageTemperature | DOUBLE      | GlobalLandTemperaturesByCity.csv| AverageTemperature | Average Temperature   |\n",
    "| TEMPERATURE_DIM  | City               | STRING      | GlobalLandTemperaturesByCity.csv| City               | City                  |\n",
    "\n",
    "The AIRPORT_DIM dimension will be a dimension table containing airport level information corresponding to a Region. The data dictionary and lineage for this table will be will be:\n",
    "    \n",
    "| Table            | Variable         | DataType    | Source File      | Source Variable  | Description           |\n",
    "| ---------------- | ---------------- | ----------- | ---------------- | ---------------- | --------------------- |\n",
    "| AIRPORT_DIM      | name             | STRING      | airport-codes_csv| name             | airport name          |\n",
    "| AIRPORT_DIM      | type             | STRING      | airport-codes_csv| type             | airport type          |\n",
    "| AIRPORT_DIM      | elevation_ft     | DOUBLE      | airport-codes_csv| elevation_ft     | Elevation (ft)        |\n",
    "| AIRPORT_DIM      | iso_country      | STRING      | airport-codes_csv| iso_country      | Country               |\n",
    "| AIRPORT_DIM      | region           | STRING      | airport-codes_csv| isoregion        | region\t\t\t      |\n",
    "| AIRPORT_DIM      | municipality     | STRING      | airport-codes_csv| municipality     | district              |  \n",
    "\n",
    " All tables will be saved to Parquet files under \"/data_model\" and will be partitioned accordingly.\n",
    "\n",
    "## 3.2 Mapping Out Data Pipelines\n",
    "The pipeline steps are described below:\n",
    "\n",
    "1. Loop through available I94 immigration files, extract into Spark dataframe, clean/transform and load to parquet\n",
    "2. Get distinct foreign key values from immigration data\n",
    "3. Extract into Spark DataFrame, Transform and Load temperature data\n",
    "4. Extract into Spark DataFrame, Transform and Load city data\n",
    "5. Extract into Spark DataFrame, Transform and Load airport data\n",
    "\n",
    "# Step 4: Run Pipelines to Model the Data\n",
    "\n",
    "## 4.1 Create the data model\n",
    "Build the data pipelines to create the data model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### 1. Loop through available I94 immigration files, read into Spark dataframe, clean/transform and load to parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing file: i94_apr16_sub.sas7bdat\n",
      "finished processing immigration file: i94_apr16_sub.sas7bdat\n",
      "processing file: i94_sep16_sub.sas7bdat\n",
      "finished processing immigration file: i94_sep16_sub.sas7bdat\n",
      "processing file: i94_nov16_sub.sas7bdat\n",
      "finished processing immigration file: i94_nov16_sub.sas7bdat\n",
      "processing file: i94_mar16_sub.sas7bdat\n",
      "finished processing immigration file: i94_mar16_sub.sas7bdat\n",
      "processing file: i94_jun16_sub.sas7bdat\n",
      "finished processing immigration file: i94_jun16_sub.sas7bdat\n",
      "processing file: i94_aug16_sub.sas7bdat\n",
      "finished processing immigration file: i94_aug16_sub.sas7bdat\n",
      "processing file: i94_may16_sub.sas7bdat\n",
      "finished processing immigration file: i94_may16_sub.sas7bdat\n",
      "processing file: i94_jan16_sub.sas7bdat\n",
      "finished processing immigration file: i94_jan16_sub.sas7bdat\n",
      "processing file: i94_oct16_sub.sas7bdat\n",
      "finished processing immigration file: i94_oct16_sub.sas7bdat\n",
      "processing file: i94_jul16_sub.sas7bdat\n",
      "finished processing immigration file: i94_jul16_sub.sas7bdat\n",
      "processing file: i94_feb16_sub.sas7bdat\n",
      "finished processing immigration file: i94_feb16_sub.sas7bdat\n",
      "processing file: i94_dec16_sub.sas7bdat\n",
      "finished processing immigration file: i94_dec16_sub.sas7bdat\n",
      "finished processing all immigration files\n"
     ]
    }
   ],
   "source": [
    "immigration_path = '../../data/18-83510-I94-Data-2016'\n",
    "\n",
    "# to run for all files uncomment the below\n",
    "immigration_files = [f for f in listdir(immegration_path) if isfile(join(immegration_path, f))]\n",
    "# immigration_files = ['i94_jan16_sub.sas7bdat', 'i94_feb16_sub.sas7bdat', 'i94_mar16_sub.sas7bdat']\n",
    "\n",
    "def extract_transform_load_immigrations(file_list: list):\n",
    "    \"\"\"\n",
    "    Takes a list of  immegration filepaths and loops through:\n",
    "        - Loading into Spark Dataframe clean\n",
    "        - Seperate out into FACT and DIMENSION data\n",
    "        - Write to parquet\n",
    "    args:\n",
    "        file_list: A list of file paths\n",
    "        \n",
    "    returns:\n",
    "        None\n",
    "    \"\"\"  \n",
    "    \n",
    "    #Loop through file lits\n",
    "    for f in file_list:\n",
    "        print('processing file: ' + f)\n",
    "        try:\n",
    "            # get full file path\n",
    "            filepath=join(immigration_path, f)\n",
    "            \n",
    "            # read and clean i94 data\n",
    "            stg_immigration = read_and_clean_i94_data(filepath)\n",
    "            \n",
    "        except:\n",
    "            raise Exception('failed to read i94 data') \n",
    "        try:\n",
    "            # Extract columns for immigration dimension and fact table\n",
    "            immigration_fact_table = stg_immigration.select([\"cicid\", \"arrdate\", \"i94port\", \"cnt\", \"airline\", \"fltno\", \"i94cit\", \"City\", \"Region\"])\n",
    "            immigration_dim_table = stg_immigration.select([\"cicid\", \"arrdate\", \"depdate\", \"i94visa\", \"occup\", \"gender\", \"visatype\", \"i94addr\"])\n",
    "            \n",
    "            # Write temperature dimension table to parquet files partitioned by arrdate\n",
    "            immigration_fact_table.write.mode(\"append\").partitionBy(\"arrdate\").parquet(\"/data_model/immigration_fact.parquet\")\n",
    "            immigration_dim_table.write.mode(\"append\").partitionBy(\"arrdate\").parquet(\"/data_model/immigration_dim.parquet\")\n",
    "        except:\n",
    "            raise Exception('failed to write fact and dimension table to parquet')\n",
    "    \n",
    "        print('finished processing immigration file: ' + f)\n",
    "    print('finished processing all immigration files')\n",
    "    \n",
    "extract_transform_load_immigrations(immigration_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### 2. Get distinct foreign key values from immigration data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# For the other dimension tables, we want to perform an inner join to the full fact table to remove all the values from the dimension tables that aren't relevant data because they won't match to the fact table. \n",
    "# This will minize the amount of data needed to be stored\n",
    "\n",
    "distinct_fact_cities = spark.read.parquet(\"/data_model/immigration_fact.parquet/\").select('City').distinct()\n",
    "distinct_fact_region = spark.read.parquet(\"/data_model/immigration_fact.parquet/\").select('Region').distinct()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### 3. Extract into Spark DataFrame, Transform and Load temperature data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Extract columns for temperature dimension table\n",
    "stg_temperature_dim_table = read_and_clean_temperature_data('../../data2/GlobalLandTemperaturesByCity.csv').alias('stg_temperature')\n",
    "\n",
    "temperature_dim_table = (\n",
    "    stg_temperature_dim_table\n",
    "    .join(distinct_fact_cities, on = ['City'], how='inner')\n",
    "    .select('stg_temperature.*')\n",
    ")\n",
    "\n",
    "temperature_dim_table.write.mode(\"overwrite\").partitionBy(\"City\").parquet(\"/data_model/temperature_dim.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### 4. Extract into Spark DataFrame, Transform and Load Cities data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Extract columns for city dimension table\n",
    "stg_city_dim_table = read_and_clean_cities_data('./us-cities-demographics.csv').alias('stg_city')\n",
    "\n",
    "city_dim_table = (\n",
    "    stg_city_dim_table\n",
    "    .join(distinct_fact_cities, on = ['City'], how='inner')\n",
    "    .select('stg_city.*')\n",
    ")\n",
    "\n",
    "city_dim_table.write.mode(\"overwrite\").parquet(\"/data_model/city_dim.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### 5. Extract into Spark DataFrame, Transform and Load airport data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Extract columns for airport dimension table\n",
    "stg_airport_dim_table = read_and_clean_airport_data('./airport-codes_csv.csv').alias('stg_airport')\n",
    "\n",
    "airport_dim_table = (\n",
    "    stg_airport_dim_table\n",
    "    .join(distinct_fact_region, on = ['Region'], how='inner')\n",
    "    .select('stg_airport.*')\n",
    ")\n",
    "\n",
    "airport_dim_table.write.mode(\"overwrite\").parquet(\"/data_model/airport_dim.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## 4.2 Data Quality Checks\n",
    "The data quality check will ensure there are adequate number of entries in each table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Quality check passed: immigration_fact exists and can be read by spark\n",
      "Data quality check passed: immigration_fact has 33658753 records\n",
      "Data Quality check passed: ['cicid'] contains no null values for table immigration_fact\n",
      "Data Quality check passed: immigration_fact has no duplicates\n",
      "Data Qualkity checks complete and passed for table immigration_fact\n",
      "Data Quality check passed: immigration_dim exists and can be read by spark\n",
      "Data quality check passed: immigration_dim has 33658753 records\n",
      "Data Quality check passed: ['cicid'] contains no null values for table immigration_dim\n",
      "Data Quality check passed: immigration_dim has no duplicates\n",
      "Data Qualkity checks complete and passed for table immigration_dim\n",
      "Data Quality check passed: temperature_dim exists and can be read by spark\n",
      "Data quality check passed: temperature_dim has 209783 records\n",
      "Data Quality check passed: ['City', 'dt'] contains no null values for table temperature_dim\n",
      "Data Quality check passed: temperature_dim has no duplicates\n",
      "Data Qualkity checks complete and passed for table temperature_dim\n",
      "Data Quality check passed: city_dim exists and can be read by spark\n",
      "Data quality check passed: city_dim has 107 records\n",
      "Data Quality check passed: ['City'] contains no null values for table city_dim\n",
      "Data Quality check passed: city_dim has no duplicates\n",
      "Data Qualkity checks complete and passed for table city_dim\n",
      "Data Quality check passed: airport_dim exists and can be read by spark\n",
      "Data quality check passed: airport_dim has 25728 records\n",
      "Data Quality check passed: ['name'] contains no null values for table airport_dim\n",
      "Data Quality check passed: airport_dim has no duplicates\n",
      "Data Qualkity checks complete and passed for table airport_dim\n"
     ]
    }
   ],
   "source": [
    "def perform_data_quality_checks(table_name, primary_key=None):\n",
    "    '''\n",
    "    Input: Spark dataframe, description of Spark dataframe\n",
    "    \n",
    "    Output: Print outcome of data quality check\n",
    "    \n",
    "    '''\n",
    "    parquet_path = \"/data_model/\" + table_name + \".parquet/\"\n",
    "    # first check table exists\n",
    "    try:\n",
    "        df = spark.read.parquet(parquet_path)\n",
    "        print(f'Data Quality check passed: {table_name} exists and can be read by spark')\n",
    "    except:\n",
    "        raise Exception(f'Data Quality check failed: table does not exist or failed to read {table_name}')\n",
    "    \n",
    "    # Next check table is not empty\n",
    "    try:\n",
    "        result = df.count() \n",
    "        if result == 0:\n",
    "            raise Exception(f'Data quality check failed: {table_name} has zero rows')\n",
    "        else:\n",
    "            print(f'Data quality check passed: {table_name} has {result} records')\n",
    "    except:\n",
    "        raise Exception(f'Data quality check failed: Failed to run row count')\n",
    "    \n",
    "    # Next check the primary key is valid\n",
    "    if primary_key != None:\n",
    "        try:\n",
    "            result = df[primary_key].count() - df.dropna(subset=primary_key).count()\n",
    "            if result >0:\n",
    "                raise Exception(f'Data Quality check failed: {primary_key} contains null values for table {table_name}')\n",
    "            else:\n",
    "                print(f'Data Quality check passed: {primary_key} contains no null values for table {table_name}')\n",
    "        except:\n",
    "            raise Exception(f'Data quality check failed: Failed to null value primary key check for table {table_name}')\n",
    "        try:\n",
    "            df.count() - df.dropDuplicates(primary_key).count()\n",
    "            if result > 0:\n",
    "                raise Exception(f'Data Quality check failed: {table_name} has duplicates for primary key {primary_key}')\n",
    "            else:\n",
    "                print(f'Data Quality check passed: {table_name} has no duplicates')\n",
    "        except:\n",
    "            raise Exception((f'Data Quality check failed: failed to run duplicates check'))\n",
    "            \n",
    "    return print(f'Data Qualkity checks complete and passed for table {table_name}')\n",
    "    \n",
    "# Perform data quality check\n",
    "perform_data_quality_checks(\"immigration_fact\", ['cicid'])\n",
    "perform_data_quality_checks(\"immigration_dim\", ['cicid'])\n",
    "perform_data_quality_checks(\"temperature_dim\", ['City','dt'])\n",
    "perform_data_quality_checks(\"city_dim\", ['City'])\n",
    "perform_data_quality_checks(\"airport_dim\", ['name'])\n",
    "                      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## 4.3 Data dictionary\n",
    "See Section 3.1 for data dictionaries\n",
    "\n",
    "# Step 5: Complete Project Write Up\n",
    "\n",
    "## Clearly state the rationale for the choice of tools and technologies for the project.\n",
    "After performing some basic EDA and analysising the size of the data, Spark was chosen over Pandas because of it's efficiency benefits when processing large ammounts of data and it's ability to support multiple file formats (including .csv, .txt and .sas7bdat). Pyspark was chosen as a the programming tool/language throughout the pipeline in order to remain consistent and avoid changing between pyspark, sparkSQL and pyspark. The workspace resources proved capabale for handling the volume of data and given time constraints and that all the data was already present in the workspace, I avoided using cloud services (but this would be a consideration in the future to further expand my skillset).\n",
    "\n",
    "## Propose how often the data should be updated and why.\n",
    "Because the immigration files appear to be delivered in monthly batches, I propose this pipeline should be run on a monthly basis. There may be a requirement to update the airport, cities and temperature data on an ad-hoc or more frequent basis.\n",
    "\n",
    "## Write a description of how you would approach the problem differently under the following scenarios:\n",
    "\n",
    "### The data was increased by 100x.\n",
    "If the data was increased by 100x, it is unlikely we would be able to continue to efficiently process the data using the available workspace resources, unless we were to break the data down into smaller chunks but then we may result in inefficiencies due to the number of partitions. It is likely we would consider using a remote spark cluster mode (with a cluster manager such as Yarn). This could be part of a business' exist on-premise infrastructure or exist in a public or private cloud (such as AWS, GCP or Azure).\n",
    "\n",
    "### The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    "Currently this pipeline is being manually executed, so it is probably not suitable for a 7am SLA. A scheduling tool such as Airflow would be suitable for this scenario and would allow the pipeline to either run on a schedule or be triggered by the data updates.\n",
    "\n",
    "### The database needed to be accessed by 100+ people.\n",
    "If the database needed to be accessed by 100+ people, we could consider rendering the parquet files to HDFS and so that we could control user permission and give read access to the users that need it. We could also use technology such as HIVE to build a SQL-like layer on top of the data, allowing for for users with various skillsets to leverage the data. We could also consider other Big Data technology such a Google BigQuery or AWS Redshift."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
